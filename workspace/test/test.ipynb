{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20210101 00', '20210101 01', '20210101 02', '20210101 03', '20210101 04', '20210101 05', '20210101 06', '20210101 07', '20210101 08', '20210101 09']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "mindate = datetime.strptime('2021-01-01','%Y-%m-%d')\n",
    "maxdate = datetime.strptime('2021-12-31','%Y-%m-%d')\n",
    "daterange = [ (mindate + timedelta(hours=x)).strftime('%Y%m%d %H') for x in range(0,(maxdate-mindate).days)]\n",
    "\n",
    "print(daterange[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-01-09', '2021-01-10']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dt_index = pd.date_range(start='20210101', end='20211231')\n",
    "dt_list = dt_index.strftime('%Y-%m-%d').tolist()\n",
    "print(dt_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f3f1c425af0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unexpected item type: <class 'slice'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/test.ipynb Cell 3'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=8'>9</a>\u001b[0m attr \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=10'>11</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=11'>12</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39mparquet\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mTrue\u001b[39;00m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m'\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39mload(data_path) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=15'>16</a>\u001b[0m     \u001b[39m.\u001b[39mselectExpr(attr)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c5f6a7570797465725f31227d/home/jovyan/work/test.ipynb#ch0000002vscode-remote?line=16'>17</a>\u001b[0m df[:\u001b[39m3\u001b[39;49m]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1646\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/spark/python/pyspark/sql/dataframe.py?line=1643'>1644</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(jc)\n\u001b[1;32m   <a href='file:///usr/local/spark/python/pyspark/sql/dataframe.py?line=1644'>1645</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///usr/local/spark/python/pyspark/sql/dataframe.py?line=1645'>1646</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munexpected item type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(item))\n",
      "\u001b[0;31mTypeError\u001b[0m: unexpected item type: <class 'slice'>"
     ]
    }
   ],
   "source": [
    "# tier1 데이터 sql\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('test-session') \\\n",
    "            .master('local[*]') \\\n",
    "            .getOrCreate()\n",
    "print(spark)\n",
    "data_path = '/home/data/data-warehouse/tier1/event/2020-12-01/00'\n",
    "attr = [\n",
    "    '*',\n",
    "]\n",
    "df = spark.read.format('parquet') \\\n",
    "    .option('header',True) \\\n",
    "    .option('compression', 'gzip') \\\n",
    "    .load(data_path) \\\n",
    "    .selectExpr(attr)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fb6e5f08e20>\n",
      "root\n",
      " |-- tracking: string (nullable = false)\n",
      " |-- trackingId: string (nullable = true)\n",
      " |-- trackingEventCode: string (nullable = false)\n",
      " |-- cid: string (nullable = true)\n",
      " |-- osTypeCode: string (nullable = false)\n",
      " |-- logTimeStamp: timestamp (nullable = true)\n",
      " |-- eventTimeStamp: timestamp (nullable = true)\n",
      " |-- campaign: string (nullable = false)\n",
      " |-- contentId: string (nullable = true)\n",
      " |-- contentName: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- activityParam: string (nullable = true)\n",
      " |-- attributed: string (nullable = false)\n",
      " |-- latdAdvertisingPartnerName: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw 데이터 sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('test-session') \\\n",
    "            .master('local[*]') \\\n",
    "            .getOrCreate()\n",
    "print(spark)\n",
    "data_path = '/home/data/dmc-integrated-analytics/rawdata/thirdparty-branch/2020-12/01/00/*'\n",
    "# data_path = '/home/data/dmc-integrated-analytics/rawdata/thirdparty-adbrix/2020-12/01/00/*'\n",
    "# data_path = '/home/data/dmc-integrated-analytics/rawdata/thirdparty-singular/2021-01/01/00/*'\n",
    "pre_branch_attr = [\n",
    "    '\"BRANCH\"                                        AS tracking',\n",
    "    'trackingObject.organization_name[0]             AS trackingId',\n",
    "    'CASE WHEN trackingObject.name[0] = \"SEARCH\"   THEN \"TPD007\" \\\n",
    "            WHEN trackingObject.name[0] = \"VIEW_ITEM\"  THEN \"TPD002\" \\\n",
    "            WHEN trackingObject.name[0] = \"ADD_TO_CART\" THEN \"TPD003\" \\\n",
    "            WHEN trackingObject.name[0] = \"PURCHASE\" THEN \"TPD004\"\\\n",
    "            WHEN trackingObject.name[0] = \"INSTALL\"    THEN \"TPD008\"\\\n",
    "            WHEN trackingObject.name[0] = \"OPEN\" THEN \"TPD009\"  ELSE \"TPD999\" END     AS trackingEventCode',\n",
    "    'CASE WHEN trackingObject.ud_os[0] = \"ANDROID\" THEN trackingObject.ud_aaid[0] \\\n",
    "            WHEN trackingObject.ud_os[0] = \"IOS\" THEN trackingObject.ud_idfa[0] END                     AS cid',\n",
    "    'CASE WHEN trackingObject.ud_os[0] = \"ANDROID\" THEN \"ATC001\" \\\n",
    "            WHEN trackingObject.ud_os[0] = \"IOS\" THEN \"ATC002\" ELSE \"ATC999\" END      AS osTypeCode',\n",
    "    'logTimeStamp                  ',\n",
    "    'trackingObject.event_timestamp[0]               AS eventTimeStamp',\n",
    "    'trackingObject.latd_campaign[0]                 AS campaign',\n",
    "    'trackingObject.ci0_product_name[0]              AS contentId',\n",
    "    '\"\"                                              AS contentName',\n",
    "    'trackingObject.ci0_price[0]                     AS value',\n",
    "    'trackingObject.ci0_quantity[0]                  AS quantity',\n",
    "    'trackingObject.ci0_price[0] * trackingObject.ci0_quantity[0]     AS amount',\n",
    "    'trackingObject.ci0_currency[0]                  AS currency',\n",
    "    '\"\"                                              AS activityParam',\n",
    "    'trackingObject.attributed[0]                    AS attributed',\n",
    "    'trackingObject.latd_advertising_partner_name[0] AS latdAdvertisingPartnerName',\n",
    "]\n",
    "pre_adbrix_attr = [  \n",
    "    '\"ADBRIX\"                                      AS tracking',\n",
    "    'trackingObject.package_name[0]                AS trackingId',      \n",
    "    'CASE WHEN trackingObject.activity[0] = \"search\"  THEN \"TPD001\" \\\n",
    "            WHEN trackingObject.activity[0] = \"product_view\"  THEN \"TPD002\" \\\n",
    "            WHEN trackingObject.activity[0] = \"add_to_cart\" THEN \"TPD003\" \\\n",
    "            WHEN trackingObject.activity[0] = \"purchase\" THEN \"TPD004\" \\\n",
    "            WHEN trackingObject.activity[0] = \"sign_up\" THEN \"TPD012\" \\\n",
    "            WHEN trackingObject.activity[0] = \"refund\" THEN \"TPD015\" \\\n",
    "            WHEN trackingObject.activity[0] = \"add_to_wishlist\" THEN \"TPD014\" \\\n",
    "            WHEN trackingObject.activity[0] = \"login\" THEN \"TPD013\" ELSE \"TPD999\" END  AS trackingEventCode',\n",
    "    'CASE WHEN trackingObject.platform[0] = \"android\" THEN trackingObject.gaid[0] ELSE trackingObject.ifa[0] END AS cid',\n",
    "    'CASE WHEN trackingObject.platform[0] = \"android\" THEN \"ATC001\" \\\n",
    "            WHEN trackingObject.platform[0] = \"ios\" THEN \"ATC002\" ELSE \"ATC999\" END AS osTypeCode', \n",
    "    'CAST(logTimeStamp AS timestamp) AS logTimeStamp',\n",
    "    'CAST(trackingObject.event_time[0] * 1000 AS timestamp)    AS eventTimeStamp',\n",
    "    '\"\"                                            as campaign',\n",
    "    'trackingObject.product_id[0]                  AS contentId',\n",
    "    'trackingObject.product_name[0]                AS contentName',\n",
    "    'cast(trackingObject.price[0] as integer)                       AS value',\n",
    "    'cast(trackingObject.quantity[0] as integer)                    AS quantity',\n",
    "    'cast(trackingObject.sales[0] as integer)                       AS amount',\n",
    "    'trackingObject.currency[0]                    AS currency',\n",
    "    'trackingObject.activity_param[0]              AS activityParam',\n",
    "    '\"\"                                            AS attributed',\n",
    "    '\"\"                                            AS latdAdvertisingPartnerName',      \n",
    "]\n",
    "pre_singular_attr = [\n",
    "    '\"SINGULAR\"                                               AS tracking',\n",
    "    'trackingObject.app_name[0]                               AS trackingid',\n",
    "    'CASE WHEN trackingObject.evtname[0] = \"ViewItem\"  THEN \"TPD002\"\\\n",
    "            WHEN trackingObject.evtname[0] = \"AddToCart\" THEN \"TPD003\"\\\n",
    "            WHEN trackingObject.evtname[0] = \"Purchase\"  THEN \"TPD004\"\\\n",
    "            WHEN trackingObject.evtname[0] = \"FirstPurchase\"  THEN \"TPD010\"\\\n",
    "            WHEN trackingObject.evtname[0] = \"__START__\" THEN \"TPD008\"\\\n",
    "            WHEN trackingObject.evtname[0] = \"__SESSION__\"  THEN \"TPD009\"\\\n",
    "            WHEN trackingObject.evtname[0] = \"__iap__\"   THEN \"TPD011\"   ELSE \"TPD999\" END AS trackingEventCode',\n",
    "    'CASE WHEN trackingObject.platform[0] = \"Android\" THEN trackingObject.aifa[0] ELSE trackingObject.idfa[0] END AS cid',\n",
    "    'CASE WHEN trackingObject.platform[0] = \"Android\" THEN \"ATC001\"\\\n",
    "            WHEN trackingObject.platform[0] = \"iOS\" THEN \"ATC002\"  ELSE \"ATC999\" END  AS osTypeCode',\n",
    "    'logTimeStamp',\n",
    "    'CAST(trackingObject.utc[0] * 1000 AS String)             AS eventTimeStamp',\n",
    "    'trackingObject.cid[0]                                    AS campaign',\n",
    "    'CASE WHEN trackingObject.evtname[0] = \"__iap__\" THEN GET_JSON_OBJECT(trackingObject.evtattr[0], \"$[0].id\") ELSE GET_JSON_OBJECT(trackingObject.evtattr[0], \"$[0].prd_code\") END AS contentId',\n",
    "    'GET_JSON_OBJECT(trackingObject.evtattr[0], \"$[0].product\")  AS contentName',\n",
    "    'CASE WHEN trackingObject.evtname[0] = \"__iap__\" THEN GET_JSON_OBJECT(trackingObject.evtattr[0], \"$[0].price\") ELSE \"0\" END  AS value',\n",
    "    'CASE WHEN trackingObject.evtname[0] = \"__iap__\" THEN GET_JSON_OBJECT(trackingObject.evtattr[0], \"$[0].quantity\") ELSE \"0\" END AS quantity',\n",
    "    'trackingObject.amount[0]                                 as amount',\n",
    "    'trackingObject.currency[0]                               as currency',\n",
    "    'trackingObject.evtattr[0]                                AS activityParam',\n",
    "    'trackingObject.is_re_eng[0]                              AS attributed',\n",
    "    '\"\"                                                       AS latdAdvertisingPartnerName',\n",
    "]\n",
    "\n",
    "# 스키마 지정해서 불러들여야하는데... 에러남\n",
    "union_schema = StructType([\n",
    "    StructField('tracking', StringType(), True),\n",
    "    StructField('trackingId', StringType(), True),\n",
    "    StructField('trackingEventCode', StringType(), True),\n",
    "    StructField('cid', StringType(), True),\n",
    "    StructField('osTypeCode', StringType(), True),\n",
    "    StructField('logTimeStamp', TimestampType(), True),\n",
    "    StructField('eventTimeStamp', TimestampType(), True),\n",
    "    StructField('campaign', StringType(), True),\n",
    "    StructField('contentId', StringType(), True),\n",
    "    StructField('contentName', StringType(), True),\n",
    "    StructField('value', IntegerType(), True),\n",
    "    StructField('quantity', IntegerType(), True),\n",
    "    StructField('amount', IntegerType(), True),\n",
    "    StructField('currency', StringType(), True),\n",
    "    StructField('activityParam', StringType(), True),\n",
    "    StructField('attributed', StringType(), True),\n",
    "    StructField('latdAdvertisingPartnerName', StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read\\\n",
    "    .format('json')\\\n",
    "    .option('header',True)\\\n",
    "    .load(data_path)\\\n",
    "    .selectExpr(pre_adbrix_attr)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tracking: string (nullable = false)\n",
      " |-- trackingId: string (nullable = true)\n",
      " |-- trackingEventCode: string (nullable = false)\n",
      " |-- cid: string (nullable = true)\n",
      " |-- osTypeCode: string (nullable = false)\n",
      " |-- logTimeStamp: timestamp (nullable = true)\n",
      " |-- eventTimeStamp: timestamp (nullable = true)\n",
      " |-- campaign: string (nullable = false)\n",
      " |-- contentId: string (nullable = true)\n",
      " |-- contentName: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- activityParam: string (nullable = true)\n",
      " |-- attributed: string (nullable = false)\n",
      " |-- latdAdvertisingPartnerName: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o95.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 2109) (0df59ca550ae executor driver): java.io.IOException: Mkdirs failed to create file:/home/data/temp/adbrix/_temporary/0/_temporary/attempt_202202230927335126710497589675418_0010_m_000000_2109 (exists=false, cwd=file:/home/jovyan)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n\t... 42 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/data/temp/adbrix/_temporary/0/_temporary/attempt_202202230927335126710497589675418_0010_m_000000_2109 (exists=false, cwd=file:/home/jovyan)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/test.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# write\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m write_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/data/temp/adbrix\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m df\u001b[39m.\u001b[39;49mwhere(\u001b[39m'\u001b[39;49m\u001b[39mtrackingId is not NULL\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39;49msort(\u001b[39m'\u001b[39;49m\u001b[39meventTimeStamp\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39;49mcoalesce(\u001b[39m20\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39;49mwrite\\\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mparquet.enable.dictionary\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mparquet.block.size\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39m128\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m1024\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m1024\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mparquet.page.size\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m1024\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m1024\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mparquet.dictionary.page.size\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39m8\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m1024\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m1024\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737061726b2d65746c2d6a7570797465722d31227d/home/jovyan/work/test.ipynb#ch0000005vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39;49mparquet(write_path)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:885\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/spark/python/pyspark/sql/readwriter.py?line=882'>883</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m    <a href='file:///usr/local/spark/python/pyspark/sql/readwriter.py?line=883'>884</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m--> <a href='file:///usr/local/spark/python/pyspark/sql/readwriter.py?line=884'>885</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/spark/python/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///usr/local/spark/python/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/spark/python/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///usr/local/spark/python/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///usr/local/spark/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o95.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 2109) (0df59ca550ae executor driver): java.io.IOException: Mkdirs failed to create file:/home/data/temp/adbrix/_temporary/0/_temporary/attempt_202202230927335126710497589675418_0010_m_000000_2109 (exists=false, cwd=file:/home/jovyan)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n\t... 42 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/data/temp/adbrix/_temporary/0/_temporary/attempt_202202230927335126710497589675418_0010_m_000000_2109 (exists=false, cwd=file:/home/jovyan)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# write\n",
    "write_path = '/home/data/temp/adbrix'\n",
    "df.where('trackingId is not NULL')\\\n",
    "    .sort('eventTimeStamp')\\\n",
    "    .coalesce(20)\\\n",
    "    .write\\\n",
    "    .option('compression','gzip')\\\n",
    "    .option('parquet.enable.dictionary', 'true')\\\n",
    "    .option('parquet.block.size', f'{128*1024*1024}')\\\n",
    "    .option('parquet.page.size', f'{2*1024*1024}')\\\n",
    "    .option('parquet.dictionary.page.size', f'{8*1024*1024}')\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet(write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and print schema\n",
    "df2 = spark.read\\\n",
    "    .format('parquet')\\\n",
    "    .option('header',True)\\\n",
    "    .load(write_path)    \n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('/home/data/data-warehouse/tier1/event/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
